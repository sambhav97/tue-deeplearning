{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "557bc264",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/assignments/assignment_1/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4ec9a11-3515-40aa-97ac-73c249df560e",
   "metadata": {
    "id": "a4ec9a11-3515-40aa-97ac-73c249df560e"
   },
   "source": [
    "# Group Number:\n",
    "# Student 1: Denise La Gordt Dillie\n",
    "# Student 2: Andreea Maican\n",
    "# Student 3: Sambhav Jain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8c200a7-b3ab-4c9a-bcf8-320271c040f3",
   "metadata": {
    "id": "a8c200a7-b3ab-4c9a-bcf8-320271c040f3"
   },
   "source": [
    "In case you are using google colab, uncomment the following cell, and modify the ```notebook_dir``` variable to contain the directory this notebook is in. It will automatically download the .py files needed for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "igFRsZKIC18S",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T11:56:26.426258Z",
     "start_time": "2023-05-30T11:56:26.421997Z"
    },
    "id": "igFRsZKIC18S"
   },
   "outputs": [],
   "source": [
    "# # Change the following  line to the directory this notebook is (if using colab)\n",
    "# # In case you do not know the path, open the file navigator on the left in colab\n",
    "# # Find the folder containing this notebook, then press on the three dots --> copy path\n",
    "# notebook_dir = \"/content/drive/MyDrive/Colab Notebooks/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828246d4-75b2-42b7-ab06-925e6624f411",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T11:56:35.801096Z",
     "start_time": "2023-05-30T11:56:35.788433Z"
    },
    "id": "828246d4-75b2-42b7-ab06-925e6624f411"
   },
   "outputs": [],
   "source": [
    "# # UNCOMMENT IF USING COLAB\n",
    "# from google.colab import drive\n",
    "# import requests\n",
    "# drive.mount('/content/drive')\n",
    "# import sys\n",
    "# import os\n",
    "# sys.path.insert(0, notebook_dir) \n",
    "# os.chdir(notebook_dir)\n",
    "# symco = \"https://github.com/vlamen/tue-deeplearning/blob/main/assignments/assignment_1/symconv.py?raw=true\"\n",
    "# crpt = \"https://github.com/vlamen/tue-deeplearning/blob/main/assignments/assignment_1/carpet.py?raw=true\"\n",
    "# r_s = requests.get(symco, allow_redirects=True)\n",
    "# r_c = requests.get(crpt, allow_redirects=True)\n",
    "# with open('symconv.py', 'wb') as f:\n",
    "#     f.write(r_s.content)\n",
    "# with open('carpet.py', 'wb') as f:\n",
    "#     f.write(r_c.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c401bd6-3828-4f5e-ada8-a026e0a167bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T11:56:37.850901Z",
     "start_time": "2023-05-30T11:56:36.680802Z"
    },
    "id": "1c401bd6-3828-4f5e-ada8-a026e0a167bf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "import io\n",
    "import requests\n",
    "\n",
    "import symconv as sc\n",
    "from carpet import show_carpet, oh_to_label\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c0bcb8-5215-40b3-8ba2-7e4208651c90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T11:56:39.232122Z",
     "start_time": "2023-05-30T11:56:39.221646Z"
    },
    "id": "35c0bcb8-5215-40b3-8ba2-7e4208651c90"
   },
   "outputs": [],
   "source": [
    "def load_numpy_arr_from_url(url):\n",
    "    \"\"\"\n",
    "    Loads a numpy array from surfdrive. \n",
    "    \n",
    "    Input:\n",
    "    url: Download link of dataset \n",
    "    \n",
    "    Outputs:\n",
    "    dataset: numpy array with input features or labels\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    return np.load(io.BytesIO(response.content)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "045a1fdc-8c84-4829-b8c8-14c957f733f6",
   "metadata": {
    "id": "045a1fdc-8c84-4829-b8c8-14c957f733f6"
   },
   "source": [
    "# Task 1: Pattern Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b63ab-610e-4e03-b1da-a717c2a77c8a",
   "metadata": {
    "id": "a45b63ab-610e-4e03-b1da-a717c2a77c8a"
   },
   "outputs": [],
   "source": [
    "# loading training and testing data for task 1\n",
    "# DO NOT MODIFY\n",
    "task1 = load_numpy_arr_from_url(\"https://github.com/vlamen/tue-deeplearning/blob/main/assignments/assignment_1/task1data.npz?raw=true\")\n",
    "# task1 = np.load(\"task1data.npz\")\n",
    "\n",
    "X = torch.tensor(task1['arr_0']).float()\n",
    "y = torch.tensor(task1['arr_1']).float()\n",
    "\n",
    "X_train = X[:7500]\n",
    "X_val = X[7500:9500]\n",
    "X_test = X[9500:]\n",
    "y_train = y[:7500]\n",
    "y_val = y[7500:9500]\n",
    "y_test  = y[9500:]\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "\n",
    "print(f\"Carpet train shape: {X_train.shape}\")\n",
    "print(f\"Label train shape: {y_train.shape}\")\n",
    "print(f\"Carpet validation shape: {X_val.shape}\")\n",
    "print(f\"Label validation shape: {y_val.shape}\")\n",
    "print(f\"Carpet test shape: {X_test.shape}\")\n",
    "print(f\"Label test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c5553f-023f-48fc-81b5-83184a46a21d",
   "metadata": {
    "id": "a4c5553f-023f-48fc-81b5-83184a46a21d"
   },
   "outputs": [],
   "source": [
    "# random carpet\n",
    "idx = np.random.randint(0,7500)\n",
    "show_carpet(X_train, idx)\n",
    "print('Carpet from', oh_to_label(y_train[idx,None])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, optimizer=None):\n",
    "    yb = torch.argmax(yb, dim=1)\n",
    "\n",
    "    assert yb.dim() <= 1, \"Target tensor must have 0 or 1 dimensions\"\n",
    "    assert yb.numel() == len(xb), \"Target tensor size must match input size\"\n",
    "\n",
    "    output = model(xb)\n",
    "    loss = loss_func(output, yb.long())\n",
    "\n",
    "    if optimizer is not None:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    _, preds = torch.max(output, 1)\n",
    "    corrects = torch.sum(preds == yb.long())\n",
    "\n",
    "    return loss.item(), corrects, len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fit(epochs, model, loss_func, optimizer, train_dl, valid_dl, test_dl):\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Training process\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        sample_num = 0\n",
    "\n",
    "        for xb, yb in train_dl:\n",
    "            losses, corrects, nums = loss_batch(model, loss_func, xb, yb, optimizer)\n",
    "            running_loss += losses * xb.size(0)\n",
    "            running_corrects += corrects\n",
    "            sample_num += nums\n",
    "\n",
    "        train_loss = running_loss / sample_num\n",
    "        train_acc = running_corrects.double() / sample_num\n",
    "\n",
    "        # Validation process\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            sample_num = 0\n",
    "\n",
    "            for xb, yb in valid_dl:\n",
    "                losses, corrects, nums = loss_batch(model, loss_func, xb, yb)\n",
    "                running_loss += losses * xb.size(0)\n",
    "                running_corrects += corrects\n",
    "                sample_num += nums\n",
    "\n",
    "            val_loss = running_loss / sample_num\n",
    "            val_acc = running_corrects.double() / sample_num\n",
    "\n",
    "        # Testing process\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            sample_num = 0\n",
    "\n",
    "            for xb, yb in test_dl:\n",
    "                losses, corrects, nums = loss_batch(model, loss_func, xb, yb)\n",
    "                running_loss += losses * xb.size(0)\n",
    "                running_corrects += corrects\n",
    "                sample_num += nums\n",
    "\n",
    "            test_loss = running_loss / sample_num\n",
    "            test_acc = running_corrects.double() / sample_num\n",
    "\n",
    "        # Print the results\n",
    "        print(f'EPOCH: {epoch+1:0>{len(str(epochs))}}/{epochs}', end=' ')\n",
    "        print(f'LOSS: {train_loss:.4f}', f'ACC: {train_acc:.4f} ', end=' ')\n",
    "        print(f'VAL-LOSS: {val_loss:.4f}', f'VAL-ACC: {val_acc:.4f} ', end=' ')\n",
    "        print(f'TEST-LOSS: {test_loss:.4f}', f'TEST-ACC: {test_acc:.4f} ', end='\\n')\n",
    "\n",
    "        # Save losses and accuracies\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "    # Plot losses and accuracies\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, epochs+1), train_losses, 'r:', label='Train Loss')\n",
    "    plt.plot(range(1, epochs+1), val_losses, 'g:', label='Validation Loss')\n",
    "    plt.plot(range(1, epochs+1), test_losses, 'b:', label='Test Loss')\n",
    "    plt.plot(range(1, epochs+1), train_accs, 'r', label='Train Accuracy')\n",
    "    plt.plot(range(1, epochs+1), val_accs, 'g', label='Validation Accuracy')\n",
    "    plt.plot(range(1, epochs+1), test_accs, 'b', label='Test Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss / Accuracy')\n",
    "    plt.title('Loss and Accuracy vs. Epochs')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, batch_size):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=batch_size * 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dl, valid_dl = get_data(train_dataset, val_dataset, batch_size)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    sc.Slice(rotation=4, reflection=False),\n",
    "    sc.SymmetryConv2d(1, 32, kernel_size=4, stride=4, rotation=4, reflection=False),\n",
    "    nn.ReLU(inplace=True),\n",
    "    sc.SymmetryPool(),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, stride=3),\n",
    "    nn.ReLU(inplace=True),\n",
    "    sc.Slice(rotation=4, reflection=False),\n",
    "    sc.SymmetryConv2d(64, 10, kernel_size=5, stride=1, rotation=4, reflection=False),\n",
    "    nn.ReLU(inplace=True),\n",
    "    sc.SymmetryPool(),\n",
    "    nn.BatchNorm2d(10),\n",
    "    nn.Dropout(0.2),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "    nn.Linear(40, 100),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(100, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "lr = 0.06\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "epochs = 20\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "fit(epochs, model, loss_func, optimizer, train_dl, valid_dl, test_dl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06c5a8d4",
   "metadata": {},
   "source": [
    "## Task 1: Question 5d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Experiment 1\n",
    "In this experiment, we change the number of convolutional filters in the model architecture. Convolutional filters are responsible for capturing different patterns and features from the input data. By altering the number of filters, we can assess the effect on the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Experiment 1: Change the Number of Convolutional Filters\n",
    "model_exp1 = nn.Sequential(\n",
    "    sc.Slice(rotation=4, reflection=False),\n",
    "    sc.SymmetryConv2d(1, 16, kernel_size=4, stride=4, rotation=4, reflection=False), # Change the number of filters to 16\n",
    "    nn.ReLU(inplace=True),\n",
    "    sc.SymmetryPool(),\n",
    "    nn.BatchNorm2d(16), # Adjust the batch normalization layer accordingly\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Conv2d(16, 64, kernel_size=3, stride=3),\n",
    "    nn.ReLU(inplace=True),\n",
    "    sc.Slice(rotation=4, reflection=False),\n",
    "    sc.SymmetryConv2d(64, 10, kernel_size=5, stride=1, rotation=4, reflection=False),\n",
    "    nn.ReLU(inplace=True),\n",
    "    sc.SymmetryPool(),\n",
    "    nn.BatchNorm2d(10),\n",
    "    nn.Dropout(0.2),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "    nn.Linear(40, 100),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(100, 3),\n",
    ")\n",
    "\n",
    "optimizer_exp1 = optim.SGD(model_exp1.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model_exp1, loss_func, optimizer_exp1, train_dl, valid_dl, test_dl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Experiment 2\n",
    "In this experiment, we change the activation function used in the model architecture. The activation function introduces non-linearity to the model, allowing it to learn complex patterns and make non-linear predictions. By altering the activation function, we can assess its effect on the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e8735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2: Change Activation Function\n",
    "model_exp2 = nn.Sequential(\n",
    "    sc.Slice(rotation=4, reflection=False),\n",
    "    sc.SymmetryConv2d(1, 32, kernel_size=4, stride=4, rotation=4, reflection=False),\n",
    "    nn.LeakyReLU(inplace=True),  # Change activation function\n",
    "    sc.SymmetryPool(),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, stride=3),\n",
    "    nn.LeakyReLU(inplace=True),  # Change activation function\n",
    "    sc.Slice(rotation=4, reflection=False),\n",
    "    sc.SymmetryConv2d(64, 10, kernel_size=5, stride=1, rotation=4, reflection=False),\n",
    "    nn.LeakyReLU(inplace=True),  # Change activation function\n",
    "    sc.SymmetryPool(),\n",
    "    nn.BatchNorm2d(10),\n",
    "    nn.Dropout(0.2),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "    nn.Linear(40, 100),\n",
    "    nn.LeakyReLU(inplace=True),  # Change activation function\n",
    "    nn.Linear(100, 3),\n",
    ")\n",
    "\n",
    "optimizer_exp2 = optim.SGD(model_exp2.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "fit(epochs, model_exp2, loss_func, optimizer_exp2, train_dl, valid_dl, test_dl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Experiment 3\n",
    "In this experiment, we change the learning rate used for training the model. The learning rate determines the step size at which the model updates its parameters during the optimization process. By adjusting the learning rate, we can explore its effect on the convergence speed and the overall performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Experiment 3: Adjust Learning Rate\n",
    "lr_exp3 = 0.01  # Change learning rate\n",
    "\n",
    "model_exp3 = nn.Sequential(\n",
    "    sc.Slice(rotation=4, reflection=False),\n",
    "    sc.SymmetryConv2d(1, 32, kernel_size=4, stride=4, rotation=4, reflection=False),\n",
    "    nn.ReLU(inplace=True),\n",
    "    sc.SymmetryPool(),\n",
    "    nn.BatchNorm2d(32),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, stride=3),\n",
    "    nn.ReLU(inplace=True),\n",
    "    sc.Slice(rotation=4, reflection=False),\n",
    "    sc.SymmetryConv2d(64, 10, kernel_size=5, stride=1, rotation=4, reflection=False),\n",
    "    nn.ReLU(inplace=True),\n",
    "    sc.SymmetryPool(),\n",
    "    nn.BatchNorm2d(10),\n",
    "    nn.Dropout(0.2),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "    nn.Linear(40, 100),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(100, 3),\n",
    ")\n",
    "\n",
    "optimizer_exp3 = optim.SGD(model_exp3.parameters(), lr=lr_exp3, momentum=0.9)\n",
    "\n",
    "fit(epochs, model_exp3, loss_func, optimizer_exp3, train_dl, valid_dl, test_dl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e988bc2-6ba1-49cd-ae26-6feea8ad2776",
   "metadata": {
    "id": "1e988bc2-6ba1-49cd-ae26-6feea8ad2776"
   },
   "source": [
    "# Task 2: Carpet Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a2ce3a-4c8c-4f1f-9a29-113063ce7f74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T11:56:44.915631Z",
     "start_time": "2023-05-30T11:56:43.761739Z"
    },
    "id": "20a2ce3a-4c8c-4f1f-9a29-113063ce7f74"
   },
   "outputs": [],
   "source": [
    "# loading training and testing data for task 2\n",
    "# DO NOT MODIFY\n",
    "task2 = load_numpy_arr_from_url(\"https://github.com/vlamen/tue-deeplearning/blob/main/assignments/assignment_1/task2data.npz?raw=true\")\n",
    "\n",
    "X = task2['arr_0'].astype(float)\n",
    "y = task2['arr_1'].astype(float)\n",
    "gt = task2['arr_2'].astype(float) # ground truth\n",
    "queries = task2['arr_3'].astype(float)\n",
    "targets = task2['arr_4'].astype(float)\n",
    "\n",
    "print(f\"Carpet train shape: {X.shape}\")\n",
    "print(f\"Label train shape: {y.shape}\")\n",
    "print(f\"Ground truth test shape: {gt.shape}\")\n",
    "print(f\"Query carpets shape: {queries.shape}\")\n",
    "print(f\"Candidate carpets shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ca726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"CNN Builder.\"\"\"\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "\n",
    "        self.front_layer = nn.Sequential(\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "  \n",
    "\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "\n",
    "            nn.Linear(21504, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.last_layer = nn.Linear(512, 200)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "        # conv layers\n",
    "        x = self.front_layer(x)\n",
    "        x = self.last_layer(x)\n",
    "        return x\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50179340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import BatchSampler\n",
    "import numpy as np\n",
    "class BalancedBatchSampler(BatchSampler):\n",
    "    \"\"\"\n",
    "    Returns batches of size n_classes * n_samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, labels, n_classes, n_samples):\n",
    "        self.labels = labels\n",
    "        self.labels_set = list(set(self.labels))\n",
    "        self.label_to_indices = {label: np.where(  np.array(self.labels) == label)[0]\n",
    "                                 for label in self.labels_set}\n",
    "        for l in self.labels_set:\n",
    "            np.random.shuffle(self.label_to_indices[l])\n",
    "        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n",
    "        self.count = 0\n",
    "        self.n_classes = n_classes\n",
    "        self.n_samples = n_samples\n",
    "        self.n_dataset = len(self.labels)\n",
    "        self.batch_size = self.n_samples * self.n_classes\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        while self.count + self.batch_size < self.n_dataset:\n",
    "            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n",
    "            indices = []\n",
    "            for class_ in classes:\n",
    "                indices.extend(self.label_to_indices[class_][\n",
    "                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n",
    "                                                                         class_] + self.n_samples])\n",
    "                self.used_label_indices_count[class_] += self.n_samples\n",
    "                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n",
    "                    np.random.shuffle(self.label_to_indices[class_])\n",
    "                    self.used_label_indices_count[class_] = 0\n",
    "            yield indices\n",
    "            self.count += self.n_classes * self.n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_dataset // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08fc08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.X[index]\n",
    "        label = self.y[index]\n",
    "\n",
    "        # Convert the image and label to torch tensors\n",
    "        image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "dataset = CustomDataset(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69052f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10  # Number of classes in each mini-batch\n",
    "m = 6   # Number of samples from each class\n",
    "\n",
    "train_batch_sampler = BalancedBatchSampler(y, n_classes=N, n_samples=m)\n",
    "\n",
    "triplets_train_loader = torch.utils.data.DataLoader(dataset, batch_sampler=train_batch_sampler)\n",
    "\n",
    "# for data, targets in triplets_train_loader:\n",
    "#   print(data,targets)\n",
    "#   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5623e4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "class RandomTripletSelector():\n",
    "    \"\"\"\n",
    "    Select random negative  example for  each positive pair  to create triplets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(RandomTripletSelector, self).__init__()\n",
    "\n",
    "    def get_triplets(self, embeddings, labels):\n",
    "        labels = labels.cpu().data.numpy()\n",
    "        triplets = []\n",
    "        for label in set(labels):\n",
    "            label_mask = (labels == label)\n",
    "            label_indices = np.where(label_mask)[0]\n",
    "            if len(label_indices) < 2:\n",
    "                continue\n",
    "            negative_indices = np.where(np.logical_not(label_mask))[0]\n",
    "            anchor_positives = list(combinations(label_indices, 2))  # All anchor-positive pairs\n",
    "\n",
    "            # random choose one negative example for each positive pair\n",
    "            temp_triplets = [[anchor_positive[0], anchor_positive[1], np.random.choice(negative_indices)] for anchor_positive in anchor_positives]\n",
    "            triplets += temp_triplets\n",
    "\n",
    "        return torch.LongTensor(np.array(triplets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01599cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Triplets loss\n",
    "    Takes a batch of embeddings and corresponding labels.\n",
    "    Triplets are generated using triplet_selector object that take embeddings and targets and return indices of\n",
    "    triplets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin, triplet_selector):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.triplet_selector = triplet_selector\n",
    "\n",
    "    def forward(self, embeddings, target):\n",
    "\n",
    "        triplets = self.triplet_selector.get_triplets(embeddings, target)\n",
    "\n",
    "        if embeddings.is_cuda:\n",
    "            triplets = triplets.cuda()\n",
    "\n",
    "            \n",
    "        anchor_idx= triplets[:, 0]  \n",
    "        positive_idx= triplets[:, 1]  \n",
    "        negative_idx= triplets[:, 2]  \n",
    "            \n",
    "            \n",
    "        ap_distances = (embeddings[anchor_idx] - embeddings[positive_idx]).pow(2).sum(1)  # .pow(.5)\n",
    "        an_distances = (embeddings[anchor_idx] - embeddings[negative_idx]).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(ap_distances - an_distances + self.margin)\n",
    "\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a6f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 device: torch.device,\n",
    "                 criterion: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 training_DataLoader: torch.utils.data.Dataset,\n",
    "                 epochs: int\n",
    "                 ):\n",
    "        \n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.training_DataLoader = training_DataLoader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def run_trainer(self):\n",
    "\n",
    "\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "                 \n",
    "            self.model.train()  # train mode\n",
    "\n",
    "            train_losses=[]\n",
    "            for batch in self.training_DataLoader:\n",
    "\n",
    "                x,y=batch\n",
    "                input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
    "                self.optimizer.zero_grad()  # zerograd the parameters\n",
    "                out = self.model(input)  # one forward pass\n",
    "                loss = self.criterion(out, target)  # calculate loss\n",
    "                 \n",
    "                loss_value = loss.item()\n",
    "                train_losses.append(loss_value)\n",
    "                 \n",
    "                loss.backward()  # one backward pass\n",
    "                self.optimizer.step()  # update the parameters\n",
    "                \n",
    "            self.model.eval()\n",
    "                \n",
    "            # print the results\n",
    "            print(\n",
    "                f'EPOCH: {epoch+1:0>{len(str(self.epochs))}}/{self.epochs}',\n",
    "                end=' '\n",
    "            )\n",
    "            print(f'LOSS: {np.mean(train_losses):.4f}',end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "    \n",
    "# model\n",
    "embedding_net = EmbeddingNet()\n",
    "model = embedding_net.to(device)\n",
    "\n",
    "# margin value\n",
    "margin=1\n",
    "\n",
    "# criterion\n",
    "criterion = TripletLoss(margin,  RandomTripletSelector())\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  device=device,\n",
    "                  criterion=criterion,\n",
    "                  optimizer=optimizer,\n",
    "                  training_DataLoader=triplets_train_loader,\n",
    "                  epochs=1)\n",
    "\n",
    "# start training\n",
    "trainer.run_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0cc1db-e473-412e-b6b8-c95adc5438dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:09:33.199411Z",
     "start_time": "2023-05-30T12:09:33.191652Z"
    },
    "id": "fd0cc1db-e473-412e-b6b8-c95adc5438dd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def query_performance(queries, targets, gt, top=1):\n",
    "    assert top >= 1\n",
    "    cnt = 0\n",
    "\n",
    "    # queries_np = queries.cpu().numpy() #no need to convert to numpy array\n",
    "    # targets_np = targets.cpu().numpy()\n",
    "    queries_np = queries\n",
    "    targets_np = targets\n",
    "\n",
    "\n",
    "    for i in range(gt.shape[0]):\n",
    "        q = torch.from_numpy(queries_np[i][None]).float().to(device)\n",
    "        t = torch.from_numpy(targets_np[i]).float().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            emb_q = model.get_embedding(q).cpu().numpy()  \n",
    "            emb_t = model.get_embedding(t).cpu().numpy()  \n",
    "            # emb_q = model.forward_once(q).cpu().numpy()  # Use forward_once method to get the embeddings\n",
    "            # emb_t = model.forward_once(t).cpu().numpy()  # Use forward_once method to get the embeddings\n",
    "\n",
    "            dists = pairwise_distances(emb_q, emb_t)\n",
    "\n",
    "            if top == 1:\n",
    "                pred = np.argmin(dists)\n",
    "                if pred == gt[i]:\n",
    "                    cnt += 1\n",
    "            else:\n",
    "                pred = np.argsort(dists)\n",
    "                if gt[i] in pred[0, :top].tolist():\n",
    "                    cnt += 1\n",
    "\n",
    "    return 100 * cnt / gt.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830475c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1_accuracy = query_performance(queries, targets, gt, top=1)\n",
    "print(f'top-1 accuracy: {top_1_accuracy:.2f}%')\n",
    "top_2_accuracy = query_performance(queries, targets, gt, top=2)\n",
    "print(f'top-2 accuracy: {top_2_accuracy:.2f}%')\n",
    "top_3_accuracy = query_performance(queries, targets, gt, top=3)\n",
    "print(f'top-3 accuracy: {top_3_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:07:58.427677Z",
     "start_time": "2023-05-30T12:07:58.410732Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class CarpetMatchingModel(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(CarpetMatchingModel, self).__init__()\n",
    "#         # Define your model architecture here\n",
    "#         self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.conv3 = nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "#         self.avgpool = nn.AvgPool2d(4)\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear = nn.Linear(60, num_classes)\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.relu1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = self.relu3(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = self.flatten(x)\n",
    "#         x = self.linear(x)\n",
    "#         return x\n",
    "#\n",
    "class CarpetMatchingModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CarpetMatchingModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(23040, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return torch.abs(output1 - output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d51471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarpetMatchingModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CarpetMatchingModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "                    sc.Slice(rotation=4, reflection=False),\n",
    "                    sc.SymmetryConv2d(1, 32, kernel_size=4, stride=4, rotation=4, reflection=False),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    sc.SymmetryPool(),\n",
    "                    nn.BatchNorm2d(32),\n",
    "                    nn.Dropout(0.2),\n",
    "                    nn.Conv2d(32, 64, kernel_size=3, stride=3),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    sc.Slice(rotation=4, reflection=False),\n",
    "                    sc.SymmetryConv2d(64, 10, kernel_size=5, stride=1, rotation=4, reflection=False),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    sc.SymmetryPool(),\n",
    "                    nn.BatchNorm2d(10),\n",
    "                    nn.Dropout(0.2),\n",
    "                    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "                    nn.Linear(40, 100),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Linear(100, num_classes),\n",
    "                    )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return torch.abs(output1 - output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T11:56:50.428635Z",
     "start_time": "2023-05-30T11:56:50.381450Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = CarpetMatchingModel(num_classes=200)\n",
    "loss_func = F.cross_entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert the numpy arrays to PyTorch tensors\n",
    "X_tensor = torch.from_numpy(X).float()\n",
    "y_tensor = torch.from_numpy(y).long()\n",
    "queries_tensor = torch.from_numpy(queries).float()\n",
    "targets_tensor = torch.from_numpy(targets).float()\n",
    "\n",
    "# Set the device to CUDA if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "X_tensor = X_tensor.to(device)\n",
    "y_tensor = y_tensor.to(device)\n",
    "queries_tensor = queries_tensor.to(device)\n",
    "targets_tensor = targets_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:04:23.600976Z",
     "start_time": "2023-05-30T11:58:20.403830Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Adjust batch size to be divisible by 2\n",
    "batch_size = batch_size if batch_size % 2 == 0 else batch_size - 1\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Mini-batch training\n",
    "    for i in range(0, X_tensor.size(0), batch_size):\n",
    "        inputs1 = X_tensor[i:i+batch_size:2]     # Select even indices for inputs1\n",
    "        inputs2 = X_tensor[i+1:i+batch_size+1:2]  # Select odd indices for inputs2\n",
    "\n",
    "        labels = y_tensor[i:i+batch_size:2]       # Select labels for inputs1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs1, inputs2)  # Pass different inputs to the model\n",
    "        loss = loss_func(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Store the average loss and accuracy for the epoch\n",
    "    train_loss = running_loss / (X_tensor.size(0) / batch_size)\n",
    "    train_accuracy = correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Print the average loss and accuracy for the epoch\n",
    "    print(f'EPOCH: {epoch+1:0>{len(str(num_epochs))}}/{num_epochs}', end=' ')\n",
    "    print(f'LOSS: {train_loss:.4f}', f'ACC: {train_accuracy:.4f} ', end='\\n')\n",
    "\n",
    "# Plot losses and accuracies\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, num_epochs+1), train_losses, 'r:', label='Train Loss')\n",
    "plt.plot(range(1, num_epochs+1), train_accuracies, 'r', label='Train Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss / Accuracy')\n",
    "plt.title('Loss and Accuracy vs. Epochs')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-30T12:09:41.588225Z",
     "start_time": "2023-05-30T12:09:36.344910Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_1_accuracy = query_performance(queries_tensor, targets_tensor, gt, top=1)\n",
    "print(f'top-1 accuracy: {top_1_accuracy:.2f}%')\n",
    "top_2_accuracy = query_performance(queries_tensor, targets_tensor, gt, top=2)\n",
    "print(f'top-2 accuracy: {top_2_accuracy:.2f}%')\n",
    "top_3_accuracy = query_performance(queries_tensor, targets_tensor, gt, top=3)\n",
    "print(f'top-3 accuracy: {top_3_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
